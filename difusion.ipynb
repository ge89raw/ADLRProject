{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "import ddpm\n",
    "import datasets\n",
    "\n",
    "import os\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version Installed: 2.0.1+cu117\n",
      "Torchvision version Installed: 0.15.2+cu117\n",
      "\n",
      "you are using an another version of PyTorch. We expect PyTorch 1.11.0. You may continue using your version but it might cause dependency and compatibility issues.\n",
      "you are using an another version of torchvision. We expect torchvision 0.12. You can continue with your version but it might cause dependency and compatibility issues.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f\"PyTorch version Installed: {torch.__version__}\\nTorchvision version Installed: {torchvision.__version__}\\n\")\n",
    "if not torch.__version__.startswith(\"1.11\"):\n",
    "    print(\"you are using an another version of PyTorch. We expect PyTorch 1.11.0. You may continue using your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "if not torchvision.__version__.startswith(\"0.12\"):\n",
    "    print(\"you are using an another version of torchvision. We expect torchvision 0.12. You can continue with your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATASET & DATALOADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA NORMALIZATION FUNCTION PRIOR TO DATASET GENERATION\n",
    "\n",
    "# Import dataset class\n",
    "from grasp_object_dataset import graspDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function to obtain mean, std, max, and min of given dataset\n",
    "def get_mean_std(main_dir, object_dir, dataset):\n",
    "\n",
    "    # Allocate variables\n",
    "    full_joints = []\n",
    "\n",
    "    # Load all samples - only joint values!\n",
    "    for sample in tqdm(dataset):\n",
    "        joints = sample[0]\n",
    "        full_joints.append(joints)\n",
    "\n",
    "    mean = np.mean(full_joints, axis = 0)\n",
    "    std = np.std(full_joints, axis = 0)\n",
    "    max = np.max(full_joints, axis = 0)\n",
    "    min = np.min(full_joints, axis = 0)\n",
    "    \n",
    "\n",
    "    return mean, std, max, min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b15d0d11848447a868726df43e8ecbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT NORMALIZED:  [array([ 0.0392823 ,  0.31361202,  0.70816582,  0.29692368,  0.07207266,\n",
      "        0.31826818,  0.49531689,  0.22077443,  0.1136933 ,  0.30901805,\n",
      "        0.71981804,  0.18223994,  0.24598885, -0.00452761,  0.15958449,\n",
      "        0.59792489,  0.23905943,  0.38096319,  0.97673997, -0.0487463 ,\n",
      "       -0.39349605, -0.07124564, -0.17433854,  0.36593643, -2.36187123,\n",
      "       -0.05151258, -0.05443905, -0.0173604 ]), array([0.02517107, 0.09351697, 0.02072358, 0.28532102, 0.00295238,\n",
      "       0.04999459, 0.28136656, 0.22077773, 0.01399565, 0.04496688,\n",
      "       0.36168404, 0.18209226, 0.17415622, 0.04522972, 0.06109539,\n",
      "       0.2885344 , 0.23899754, 0.07810427, 0.10572949, 0.05585855,\n",
      "       0.09310748, 0.07385562, 1.37517608, 0.13581058, 0.7259894 ,\n",
      "       0.06679289, 0.10399817, 0.01794473]), array([ 6.44533709e-02,  4.07128990e-01,  7.28889406e-01,  5.82244694e-01,\n",
      "        7.50250369e-02,  3.68262768e-01,  7.76683450e-01,  4.41552162e-01,\n",
      "        1.27688959e-01,  3.53984922e-01,  1.08150208e+00,  3.64332199e-01,\n",
      "        4.20145065e-01,  4.07021046e-02,  2.20679879e-01,  8.86459291e-01,\n",
      "        4.78056967e-01,  4.59067464e-01,  1.08246946e+00,  7.11224787e-03,\n",
      "       -3.00388575e-01,  2.60998355e-03,  1.20083754e+00,  5.01747014e-01,\n",
      "       -1.63588184e+00,  1.52803073e-02,  4.95591238e-02,  5.84329362e-04]), array([ 1.41112227e-02,  2.20095053e-01,  6.87442243e-01,  1.16026569e-02,\n",
      "        6.91202804e-02,  2.68273592e-01,  2.13950321e-01, -3.29715704e-06,\n",
      "        9.96976495e-02,  2.64051169e-01,  3.58134001e-01,  1.47672370e-04,\n",
      "        7.18326345e-02, -4.97573279e-02,  9.84890983e-02,  3.09390485e-01,\n",
      "        6.18838676e-05,  3.02858919e-01,  8.71010482e-01, -1.04604848e-01,\n",
      "       -4.86603528e-01, -1.45101264e-01, -1.54951462e+00,  2.30125851e-01,\n",
      "       -3.08786063e+00, -1.18305467e-01, -1.58437222e-01, -3.53051350e-02])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e08f19c8ff4db29f47476b380247a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALIZED:  [array([-1.98287501e-11, -5.39881739e-12, -2.41051935e-11, -1.77459825e-12,\n",
      "       -1.69381739e-10, -1.00043849e-11, -1.80716930e-12, -2.27301178e-12,\n",
      "       -3.57013015e-11, -1.11378962e-11, -1.36714190e-12, -2.73126854e-12,\n",
      "       -2.90856406e-12, -1.10867271e-11, -8.18539547e-12, -1.76441617e-12,\n",
      "       -2.06005962e-12, -6.37760422e-12, -4.76114792e-12, -8.91486707e-12,\n",
      "       -5.40238454e-12, -6.80687234e-12, -3.99950295e-13, -3.65162123e-12,\n",
      "       -6.82347512e-13, -7.50054596e-12, -4.78594236e-12, -2.79094934e-11]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1.])]\n"
     ]
    }
   ],
   "source": [
    "# DATASET AND DATALOADER DEFINITION\n",
    "\n",
    "# Import dataset class\n",
    "from grasp_object_dataset import graspDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Directories of dataset grasps + objects: set to a small version of it to overfit!! Full version found in dataset_XX_full\n",
    "main_dir = './dataset_grasps/'\n",
    "object_dir = './dataset_objects/'\n",
    "\n",
    "# Generate dataset with all dataset samples\n",
    "main_dataset = graspDataset(main_dir, object_dir, mode = 'train', split = {'train': 1, 'val': 0, 'test': 0}, normalization=None, transform_joint = None, transform_object = None)\n",
    "\n",
    "# Calculate mean, std, max, min of full dataset\n",
    "mean_std_max_min = list(get_mean_std(main_dir, object_dir, main_dataset))\n",
    "# Print not norm data\n",
    "print('NOT NORMALIZED: ', mean_std_max_min)\n",
    "\n",
    "# Datasets definition\n",
    "train_dataset = graspDataset(main_dir, object_dir, mode = 'train', split = {'train': 1, 'val': 0, 'test': 0}, normalization=mean_std_max_min)#, transform_joint = None, transform_object = None)\n",
    "val_dataset = graspDataset(main_dir, object_dir, mode = 'val', split = {'train': 1, 'val': 0, 'test': 0}, normalization=mean_std_max_min)#, transform_joint = None, transform_object = None)\n",
    "test_dataset = graspDataset(main_dir, object_dir, mode = 'test', split = {'train': 1, 'val': 0, 'test': 0}, normalization=mean_std_max_min)#, transform_joint = None, transform_object = None)\n",
    "\n",
    "# Comparison for normalized dataset\n",
    "mean_std_max_min_2 = list(get_mean_std(main_dir, object_dir, train_dataset))\n",
    "print('NORMALIZED: ', mean_std_max_min_2)\n",
    "\n",
    "# Dataloader definition\n",
    "train_dataloader = DataLoader(train_dataset , batch_size=64, shuffle=True, num_workers=2, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset , batch_size=64, shuffle=True, num_workers=2, drop_last=False)\n",
    "test_dataloader = DataLoader(test_dataset , batch_size=64, shuffle=True, num_workers=2, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "# DATASET INDICES\n",
    "\n",
    "# train_dataset[sample][index]\n",
    "# index = 0 --> joint angles\n",
    "# index = 1 --> label (grasp type) - one hot vector\n",
    "# index = 2 --> matrix distances\n",
    "# index = 3 --> file name\n",
    "\n",
    "print(train_dataset[5000][1])\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DIFFUSION MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS FOR TUNING\n",
    "\n",
    "# model\n",
    "hidden_size = 128\n",
    "hidden_layers = 3\n",
    "emb_size= 128\n",
    "time_emb= \"sinusoidal\"\n",
    "input_emb= \"sinusoidal\"\n",
    "\n",
    "# Noise scheduler\n",
    "num_timesteps = 50\n",
    "beta_schedule= 'linear'\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 1e-6\n",
    "\n",
    "#training\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ddpm.MLP(\n",
    "        hidden_size= hidden_size,\n",
    "        hidden_layers=hidden_layers,\n",
    "        emb_size= emb_size,\n",
    "        time_emb= time_emb,\n",
    "        input_emb= input_emb\n",
    "        )\n",
    "\n",
    "noise_scheduler = ddpm.NoiseScheduler(\n",
    "        num_timesteps=num_timesteps,\n",
    "        beta_schedule=beta_schedule)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr= learning_rate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d367628f5a74c9f9fca15fca42b5fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcf00d3c2fb4d08a4ca7c725774e20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc4ee0021ee4addb2821175ed6c7d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559e1181d105454da4455cd1e0f868f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b858438b11b3405696631ecf0388dc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b31b5c0bab4713b2a9a97fe742d958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba17a9825d64d3e896c87d9b73785bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23956aaa46814adbabd509ca71527337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m progress_bar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_dataloader))\n\u001b[1;32m      8\u001b[0m progress_bar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Variables\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     joint_angles_batch \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m] \n\u001b[1;32m     12\u001b[0m     label_one_hot \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "frames = []\n",
    "losses = []\n",
    "avg_loss = []\n",
    "print(\"Training model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(total=len(train_dataloader))\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Variables\n",
    "        joint_angles_batch = batch[0] \n",
    "        label_one_hot = batch[1]\n",
    "        mat_distances = batch[2] \n",
    "\n",
    "        # Noise and timesteps\n",
    "        noise = torch.randn(joint_angles_batch.shape)\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_timesteps, (joint_angles_batch.shape[0],)).long()\n",
    "\n",
    "        noisy = noise_scheduler.add_noise(joint_angles_batch, noise, timesteps)\n",
    "        noisy = noisy.double()\n",
    "        noise_pred = model(noisy, timesteps, label_one_hot, mat_distances)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(loss)\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n",
    "        losses.append(loss.detach().item())\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        global_step += 1\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    avg_loss.append(np.mean(losses))\n",
    "    losses = []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    if epoch % config.save_images_step == 0 or epoch == config.num_epochs - 1:\n",
    "        # generate data with the model to later visualize the learning process\n",
    "        model.eval()\n",
    "        sample = torch.randn(config.eval_batch_size, 2)\n",
    "        timesteps = list(range(len(noise_scheduler)))[::-1]\n",
    "        for i, t in enumerate(tqdm(timesteps)):\n",
    "            t = torch.from_numpy(np.repeat(t, config.eval_batch_size)).long()\n",
    "            with torch.no_grad():\n",
    "                residual = model(sample, t)\n",
    "            sample = noise_scheduler.step(residual, t[0], sample)\n",
    "        frames.append(sample.numpy())\n",
    "\n",
    "print(\"Saving model...\")\n",
    "outdir = f\"exps/{config.experiment_name}\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"{outdir}/model.pth\")\n",
    "\n",
    "print(\"Saving images...\")\n",
    "imgdir = f\"{outdir}/images\"\n",
    "os.makedirs(imgdir, exist_ok=True)\n",
    "frames = np.stack(frames)\n",
    "xmin, xmax = -6, 6\n",
    "ymin, ymax = -6, 6\n",
    "for i, frame in enumerate(frames):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(frame[:, 0], frame[:, 1])\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.savefig(f\"{imgdir}/{i:04}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "print(\"Saving loss as numpy array...\")\n",
    "np.save(f\"{outdir}/loss.npy\", np.array(losses))\n",
    "print(\"Saving frames...\")\n",
    "np.save(f\"{outdir}/frames.npy\", frames)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model...\")\n",
    "experiment_name = 'full_normalization'\n",
    "outdir = f\"exps/{experiment_name}\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"{outdir}/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL INFERENCE --> EVALUATION OF RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MLP:\n\tUnexpected key(s) in state_dict: \"conv_layers.0.weight\", \"conv_layers.0.bias\", \"conv_layers.3.weight\", \"conv_layers.3.bias\", \"conv_layers.6.weight\", \"conv_layers.6.bias\", \"conv_layers.10.weight\", \"conv_layers.10.bias\". \n\tsize mismatch for joint_mlp.0.weight: copying a param with shape torch.Size([128, 4227]) from checkpoint, the shape in current model is torch.Size([128, 3715]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load a model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m ddpm\u001b[39m.\u001b[39mMLP(\n\u001b[1;32m      4\u001b[0m         hidden_size\u001b[39m=\u001b[39m hidden_size,\n\u001b[1;32m      5\u001b[0m         hidden_layers\u001b[39m=\u001b[39mhidden_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         input_emb\u001b[39m=\u001b[39m input_emb\n\u001b[1;32m      9\u001b[0m         )\n\u001b[0;32m---> 12\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m./model_norm.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     13\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MLP:\n\tUnexpected key(s) in state_dict: \"conv_layers.0.weight\", \"conv_layers.0.bias\", \"conv_layers.3.weight\", \"conv_layers.3.bias\", \"conv_layers.6.weight\", \"conv_layers.6.bias\", \"conv_layers.10.weight\", \"conv_layers.10.bias\". \n\tsize mismatch for joint_mlp.0.weight: copying a param with shape torch.Size([128, 4227]) from checkpoint, the shape in current model is torch.Size([128, 3715])."
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Load a model\n",
    "\n",
    "model = ddpm.MLP(\n",
    "        hidden_size= hidden_size,\n",
    "        hidden_layers=hidden_layers,\n",
    "        emb_size= emb_size,\n",
    "        time_emb= time_emb,\n",
    "        input_emb= input_emb\n",
    "        )\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('./model_norm.pth'))\n",
    "model.eval()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for grasping validation\n",
    "main_dir = './dataset_grasps/'\n",
    "object_dir = './dataset_objects/'\n",
    "\n",
    "# Grasp code for which we have overfitting\n",
    "grasp_code = 'core-bottle-1ae823260851f7d9ea600d1a6d9f6e07'\n",
    "\n",
    "# Load matrix distance for given object\n",
    "matriz_distancias = np.load(os.path.join(object_dir+grasp_code+\".npy\"), allow_pickle=True)*0.11\n",
    "matriz_distancias = torch.from_numpy(matriz_distancias)\n",
    "matriz_distancias = matriz_distancias.reshape(1, 50, 50, 50)\n",
    "\n",
    "# Define matrix for given object\n",
    "label = torch.tensor([1, 0, 0])\n",
    "label = label.reshape(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b0c2be724b406e8678ee2a6af99ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgual/ADLRProject/ddpm.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor= torch.tensor(label, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Model inference\n",
    "model.eval()\n",
    "\n",
    "eval_batch_size = 1\n",
    "num_timesteps = 50\n",
    "plot_step = 5\n",
    "\n",
    "noise_scheduler = ddpm.NoiseScheduler(num_timesteps=num_timesteps)\n",
    "sample = torch.randn(eval_batch_size, 28)\n",
    "timesteps = list(range(num_timesteps))[::-1]\n",
    "\n",
    "samples = []\n",
    "steps = []\n",
    "\n",
    "for i, t in enumerate(tqdm(timesteps)):\n",
    "    t = torch.from_numpy(np.repeat(t, eval_batch_size)).long()\n",
    "    with torch.no_grad():\n",
    "        residual = model(sample, t, label, matriz_distancias)\n",
    "        \n",
    "    sample = noise_scheduler.step(residual, t[0], sample)\n",
    "    if (i + 1) % plot_step == 0:\n",
    "        samples.append(sample.numpy())\n",
    "        steps.append(i + 1)\n",
    "\n",
    "# Generated test grasp is saved in samples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset[0][0]\n",
    "reference = train_dataset[5000][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06445337  0.22009505  0.72888941  0.58224469  0.07502504  0.26827359\n",
      "  0.77668345  0.44155216  0.12768896  0.35398492  1.08150208  0.3643322\n",
      "  0.07183263  0.0407021   0.0984891   0.88645929  0.47805697  0.30285892\n",
      "  0.87101048 -0.10460485 -0.30038857  0.00260998 -1.54951462  0.23012585\n",
      " -1.63588184 -0.11830547  0.04955912 -0.03530513]\n",
      "[ 5.27972297e-02  2.74669576e-01  7.19391346e-01  4.52607628e-01\n",
      "  7.36331851e-02  2.95333031e-01  6.39269559e-01  3.34300967e-01\n",
      "  1.20497748e-01  3.30504924e-01  8.91944314e-01  2.75071193e-01\n",
      "  1.55367414e-01  1.84925590e-02  1.29035097e-01  7.43087555e-01\n",
      "  3.58990999e-01  3.36961760e-01  9.21702914e-01 -7.52650879e-02\n",
      " -3.50355374e-01 -3.07750932e-02 -8.53011287e-01  2.95580730e-01\n",
      " -1.94382339e+00 -8.31269499e-02  1.39922328e-03 -2.70949048e-02]\n",
      "[ 0.01165614 -0.05457452  0.00949806  0.12963707  0.00139185 -0.02705944\n",
      "  0.13741389  0.1072512   0.00719121  0.02348     0.18955777  0.08926101\n",
      " -0.08353478  0.02220955 -0.030546    0.14337174  0.11906597 -0.03410284\n",
      " -0.05069243 -0.02933976  0.0499668   0.03338508 -0.69650333 -0.06545488\n",
      "  0.30794155 -0.03517852  0.0481599  -0.00821023]\n"
     ]
    }
   ],
   "source": [
    "# Print of joint angles for predicted grasp\n",
    "reference = (reference + 1)/2\n",
    "reference = reference * (mean_std_max_min[2]- mean_std_max_min[3]) + mean_std_max_min[3]\n",
    "print(reference)\n",
    "\n",
    "# Set sample to predicted test sample\n",
    "sample = samples[-1][0]\n",
    "\n",
    "\n",
    "# Return normalized values to the initial values\n",
    "sample = (sample + 1)/2\n",
    "sample = sample * (mean_std_max_min[2]- mean_std_max_min[3]) + mean_std_max_min[3]\n",
    "\n",
    "print(sample)\n",
    "\n",
    "\n",
    "print(reference - sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VISUALIZATION OF RESULT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgual/.local/lib/python3.9/site-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from utils.hand_model_lite import HandModelMJCFLite\n",
    "import numpy as np\n",
    "import transforms3d\n",
    "import torch\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_path = './test_files/meshdata/'\n",
    "data_path = './test_files/dexgraspnet/'\n",
    "\n",
    "\n",
    "use_visual_mesh = False\n",
    "\n",
    "hand_file = \"mjcf/shadow_hand_vis.xml\" if use_visual_mesh else \"mjcf/shadow_hand_wrist_free.xml\"\n",
    "\n",
    "joint_names = [\n",
    "    'robot0:FFJ3', 'robot0:FFJ2', 'robot0:FFJ1', 'robot0:FFJ0',\n",
    "    'robot0:MFJ3', 'robot0:MFJ2', 'robot0:MFJ1', 'robot0:MFJ0',\n",
    "    'robot0:RFJ3', 'robot0:RFJ2', 'robot0:RFJ1', 'robot0:RFJ0',\n",
    "    'robot0:LFJ4', 'robot0:LFJ3', 'robot0:LFJ2', 'robot0:LFJ1', 'robot0:LFJ0',\n",
    "    'robot0:THJ4', 'robot0:THJ3', 'robot0:THJ2', 'robot0:THJ1', 'robot0:THJ0'\n",
    "]\n",
    "translation_names = ['WRJTx', 'WRJTy', 'WRJTz']\n",
    "rot_names = ['WRJRx', 'WRJRy', 'WRJRz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid geometry type box.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load hand file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hand_model \u001b[39m=\u001b[39m HandModelMJCFLite(\n\u001b[1;32m      3\u001b[0m     hand_file,\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mmjcf/meshes\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/ADLRProject/utils/hand_model_lite.py:30\u001b[0m, in \u001b[0;36mHandModelMJCFLite.__init__\u001b[0;34m(self, mjcf_path, mesh_path, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mCreate a Lite Hand Model for a MJCF robot\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m    device for torch tensors\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchain \u001b[39m=\u001b[39m pk\u001b[39m.\u001b[39;49mbuild_chain_from_mjcf(\n\u001b[1;32m     31\u001b[0m     \u001b[39mopen\u001b[39;49m(mjcf_path)\u001b[39m.\u001b[39;49mread())\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dofs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mget_joint_parameter_names())\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmesh \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_kinematics/mjcf.py:68\u001b[0m, in \u001b[0;36mbuild_chain_from_mjcf\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     63\u001b[0m root_body \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mworldbody\u001b[39m.\u001b[39mbody[\u001b[39m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m root_frame \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mFrame(root_body\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_frame\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m                          link\u001b[39m=\u001b[39mframe\u001b[39m.\u001b[39mLink(root_body\u001b[39m.\u001b[39mname,\n\u001b[1;32m     66\u001b[0m                                          offset\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mTransform3d(rot\u001b[39m=\u001b[39mroot_body\u001b[39m.\u001b[39mquat, pos\u001b[39m=\u001b[39mroot_body\u001b[39m.\u001b[39mpos)),\n\u001b[1;32m     67\u001b[0m                          joint\u001b[39m=\u001b[39mframe\u001b[39m.\u001b[39mJoint())\n\u001b[0;32m---> 68\u001b[0m _build_chain_recurse(root_frame, root_body)\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m chain\u001b[39m.\u001b[39mChain(root_frame)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_kinematics/mjcf.py:26\u001b[0m, in \u001b[0;36m_build_chain_recurse\u001b[0;34m(parent_frame, parent_body)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build_chain_recurse\u001b[39m(parent_frame, parent_body):\n\u001b[0;32m---> 26\u001b[0m     parent_frame\u001b[39m.\u001b[39mlink\u001b[39m.\u001b[39mvisuals \u001b[39m=\u001b[39m geoms_to_visuals(parent_body\u001b[39m.\u001b[39;49mgeom)\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m parent_body\u001b[39m.\u001b[39mbody:\n\u001b[1;32m     28\u001b[0m         n_joints \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(b\u001b[39m.\u001b[39mjoint)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_kinematics/mjcf.py:20\u001b[0m, in \u001b[0;36mgeoms_to_visuals\u001b[0;34m(geom)\u001b[0m\n\u001b[1;32m     18\u001b[0m         param \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid geometry type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m g\u001b[39m.\u001b[39mtype)\n\u001b[1;32m     21\u001b[0m     visuals\u001b[39m.\u001b[39mappend(frame\u001b[39m.\u001b[39mVisual(offset\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mTransform3d(rot\u001b[39m=\u001b[39mg\u001b[39m.\u001b[39mquat, pos\u001b[39m=\u001b[39mg\u001b[39m.\u001b[39mpos), geom_type\u001b[39m=\u001b[39mg\u001b[39m.\u001b[39mtype, geom_param\u001b[39m=\u001b[39mparam))\n\u001b[1;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m visuals\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid geometry type box."
     ]
    }
   ],
   "source": [
    "# Load hand file\n",
    "hand_model = HandModelMJCFLite(\n",
    "    hand_file,\n",
    "    \"mjcf/meshes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grasp original code and object\n",
    "grasp_code = 'core-bottle-1ae823260851f7d9ea600d1a6d9f6e07'\n",
    "grasp_data = np.load(\n",
    "    os.path.join(data_path, grasp_code+\".npy\"), allow_pickle=True)\n",
    "object_mesh_origin = trimesh.load(os.path.join(\n",
    "    mesh_path, grasp_code, \"coacd/decomposed.obj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'robot0:FFJ3': -0.24548496305942535, 'robot0:FFJ2': 0.6087559461593628, 'robot0:FFJ1': 0.7188290357589722, 'robot0:FFJ0': 0.2019304633140564, 'robot0:MFJ3': -0.17577102780342102, 'robot0:MFJ2': 0.38283923268318176, 'robot0:MFJ1': 1.1127177476882935, 'robot0:MFJ0': 0.403972327709198, 'robot0:RFJ3': -0.07210764288902283, 'robot0:RFJ2': 0.1400628387928009, 'robot0:RFJ1': 0.9195994138717651, 'robot0:RFJ0': 0.41342633962631226, 'robot0:LFJ4': 0.18373392522335052, 'robot0:LFJ3': 0.025994563475251198, 'robot0:LFJ2': 0.13054056465625763, 'robot0:LFJ1': 0.9286351203918457, 'robot0:LFJ0': 0.47552821040153503, 'robot0:THJ4': -0.0618005096912384, 'robot0:THJ3': 1.1335370540618896, 'robot0:THJ2': 0.041698239743709564, 'robot0:THJ1': -0.13501165807247162, 'robot0:THJ0': -2.997061756104813e-06, 'WRJRx': 3.0913850314695135, 'WRJRy': 0.2305903394170629, 'WRJRz': -2.3657796059005416, 'WRJTx': -0.043085746467113495, 'WRJTy': 0.09048712253570557, 'WRJTz': 0.1175876185297966}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hand_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m rot \u001b[39m=\u001b[39m rot[:, :\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mravel()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     24\u001b[0m hand_pose \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([qpos[name] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m translation_names] \u001b[39m+\u001b[39m rot \u001b[39m+\u001b[39m [qpos[name]\n\u001b[1;32m     25\u001b[0m                          \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m joint_names], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m hand_model\u001b[39m.\u001b[39mset_parameters(hand_pose)\n\u001b[1;32m     27\u001b[0m hand_mesh \u001b[39m=\u001b[39m hand_model\u001b[39m.\u001b[39mget_trimesh_data(\u001b[39m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m object_mesh \u001b[39m=\u001b[39m object_mesh_origin\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mapply_scale(\u001b[39m0.11\u001b[39m)\u001b[39m#grasp_data[index][\"scale\"])\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hand_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Select first grasph - could be any\n",
    "index = 0\n",
    "\n",
    "# Set sample to predicted test sample\n",
    "sample = samples[-1][0]\n",
    "\n",
    "# Return normalized values to the initial values\n",
    "sample = (sample + 1)/2\n",
    "sample = sample * (mean_std_max_min[2]- mean_std_max_min[3]) + mean_std_max_min[3]\n",
    "\n",
    "\n",
    "# Substitute values in grasp for the predicted ones\n",
    "qpos = grasp_data[index]['qpos']\n",
    "i=0\n",
    "for key in qpos:\n",
    "    qpos[key] = sample[i]\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "rot = np.array(transforms3d.euler.euler2mat(\n",
    "    *[qpos[name] for name in rot_names]))\n",
    "rot = rot[:, :2].T.ravel().tolist()\n",
    "hand_pose = torch.tensor([qpos[name] for name in translation_names] + rot + [qpos[name]\n",
    "                         for name in joint_names], dtype=torch.float, device=\"cpu\").unsqueeze(0)\n",
    "hand_model.set_parameters(hand_pose)\n",
    "hand_mesh = hand_model.get_trimesh_data(0)\n",
    "object_mesh = object_mesh_origin.copy().apply_scale(0.11)#grasp_data[index][\"scale\"])\n",
    "\n",
    "(hand_mesh+object_mesh).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
